{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks - A Tutorial\n",
    "***\n",
    "## 1. Introduction\n",
    "\n",
    "This is an introduction to [Generative Adversarial Networks (GANs)](https://arxiv.org/abs/1406.2661) and how they can be used to generate photos of handwritten digits. In particular, we will use a an architecture that is is similar to the [DCGAN](https://arxiv.org/abs/1511.06434) proposed by Radford et al.\n",
    "\n",
    "### 1.1 Core Idea\n",
    "***\n",
    "![Core Idea of GANS](imgs/GAN_Intro.jpeg)\n",
    "<sub>Image Source: https://deeplearning4j.org/generative-adversarial-network</sub>\n",
    "\n",
    "The core idea of GANs is to combine two neural networks with distinct tasks: a **generator** and a **descriminator**. The first tries to produce realistic images from random noise, while the latter tries to distinguish the generated images from the *real* images. The following analogy is taken from Goodfellow et al.'s [original paper on GANs](https://arxiv.org/abs/1406.2661):\n",
    "\n",
    "> The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles.\n",
    "\n",
    "### 1.2 Idea of this notebook\n",
    "***\n",
    "In this notebook, I just wanted to present a simple and straightforward implementation of GANs and how they can be used in practice. If you have any question, suggestions, etc., feel free to contact me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generator Network\n",
    "***\n",
    "![DCGAN Architecture](imgs/DCGAN.jpeg)\n",
    "<sub>Image Source: Original DCGAN paper (https://arxiv.org/abs/1511.06434) </sub>\n",
    "\n",
    "The generator network takes random noise of a fixed dimension, e.g. 100, as input, and slowly scales this noise by transposed convolution layers from small but many *feature maps* to 3 (RGB) or 1 (Greyscale) large feature map which is the generated image. Usually, the noise vector z is drawn from a Gaussian with mean 0 and standard deviation of 1. The idea is that we assume that real images are also drawn from a distribution, and we want to get as close as possible to this distribution with our generator. (A standard way of measure the distance of two probability distributions is the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).)\n",
    "\n",
    "Now, let's define the generator network. We adopt the findings from Radford et al. which mainly consists of \n",
    "\n",
    "1. Use Batch Normalisation\n",
    "2. Use ReLU activation in the generator network, except for the last layer where one should use tanh\n",
    "3. Only use transposed convolution operations instead of dense layers.\n",
    "\n",
    "We go from a random noise vector of dimension 100 to 256 2x2 feature maps, and then slowly scale up to the final greyscale image of size (28,28). **Please note** that we do **not** use excactly the same dimensions of the figure above as we have to adjust it to a.) our computational power and b.) our final image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, batch_size, z_dim=100, reuse=False):\n",
    "    with tf.variable_scope(\"generator\") as scope:\n",
    "        if reuse:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        \n",
    "        #Project and Reshape\n",
    "        w_d1 = tf.get_variable('g_dw1', shape=[z_dim, 4*256], initializer=tf.initializers.truncated_normal(stddev=0.2))\n",
    "        b_d1 = tf.get_variable('g_bw1', shape=[4*256], initializer=tf.initializers.zeros)\n",
    "        z_reshape = tf.matmul(z, w_d1) + b_d1\n",
    "        z_reshape = tf.nn.relu(z_reshape)\n",
    "        z_reshape = tf.reshape(z_reshape, shape=[-1, 2, 2, 256])       \n",
    "        # First Conv_transpose layer: From (2,2,265) to (4,4,64)\n",
    "        w_convt_1 = tf.get_variable('g_conv1', shape=[5, 5, 64, 256], initializer=tf.initializers.truncated_normal(stddev=0.2))\n",
    "        b_convt_1 = tf.get_variable('g_bconv1', shape=[64], initializer=tf.initializers.zeros)\n",
    "        convt_1 = tf.nn.conv2d_transpose(z_reshape, filter=w_convt_1, output_shape=[batch_size, 4, 4, 64], strides=[1,2,2,1], padding='SAME')\n",
    "        convt_1 = tf.contrib.layers.batch_norm(convt_1+b_convt_1, epsilon=1e-5, scope='bn1')\n",
    "        convt_1 = tf.nn.relu(convt_1)\n",
    "        # Second Conv_transpose layer: From (4,4,64) to (7,7,32)\n",
    "        w_convt_2 = tf.get_variable('g_conv2', shape=[3, 3, 32, 64], initializer=tf.initializers.truncated_normal(stddev=0.2))\n",
    "        b_convt_2 = tf.get_variable('g_bconv2', shape=[32], initializer=tf.initializers.zeros)\n",
    "        convt_2 = tf.nn.conv2d_transpose(convt_1, filter=w_convt_2, output_shape=[batch_size, 7, 7, 32], strides=[1,2,2,1], padding='SAME')\n",
    "        convt_2 = tf.contrib.layers.batch_norm(convt_2+b_convt_2, epsilon=1e-5, scope='bn2')\n",
    "        convt_2 = tf.nn.relu(convt_2)\n",
    "        # Third Conv_transpose layer From (7,7,32) to (14, 14, 16)\n",
    "        w_convt_3 = tf.get_variable('g_conv3', shape=[5, 5, 16, 32], initializer=tf.initializers.truncated_normal(stddev=0.2))\n",
    "        b_convt_3 = tf.get_variable('g_bconv3', shape=[16], initializer=tf.initializers.zeros)\n",
    "        convt_3 = tf.nn.conv2d_transpose(convt_2, filter=w_convt_3, output_shape=[batch_size, 14, 14, 16], strides=[1,2,2,1], padding='SAME')\n",
    "        convt_3 = tf.contrib.layers.batch_norm(convt_3+b_convt_3, epsilon=1e-5, scope='bn3')\n",
    "        convt_3 = tf.nn.relu(convt_3)\n",
    "        # Fourth Conv_transpose layer: From (14, 14, 16) to (28, 28, 1)\n",
    "        w_convt_4 = tf.get_variable('g_conv4', shape=[5, 5, 1, 16], initializer=tf.initializers.truncated_normal(stddev=0.2))\n",
    "        b_convt_4 = tf.get_variable('g_bconv4', shape=[1], initializer=tf.initializers.zeros)\n",
    "        convt_4 = tf.nn.conv2d_transpose(convt_3, filter=w_convt_4, output_shape=[batch_size, 28, 28, 1], strides=[1,2,2,1], padding='SAME')\n",
    "        convt_4 = tf.nn.tanh(convt_4)\n",
    "        return convt_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define z as a placeholder and call the generator with it so that we can generate an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dimension = 100\n",
    "z = tf.placeholder(dtype=tf.float32, shape=[None, z_dimension])\n",
    "generated_img = generator(z, 1, z_dim=z_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Generator Network - Generating an example\n",
    "***\n",
    "We can now generate an example, however, it will just be random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    z_batch = np.random.normal(-1, 1, [1, z_dimension]) # Only one example in our batch\n",
    "    feed_dict = {z: z_batch}\n",
    "    img = session.run([generated_img], feed_dict=feed_dict)\n",
    "    img = np.reshape(img, (28,28))\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Descriminator Network\n",
    "***\n",
    "![Discriminator](imgs/Discriminator.jpeg)\n",
    "<sub>Image Source: https://hackernoon.com/how-do-gans-intuitively-work-2dda07f247a1a</sub>\n",
    "\n",
    "Let's have a look at the descriminator network. It takes either the generated (fake) images or the real images as input and produces a probability how likely the input image is real. As a result, the output of this network is a single scalar between 0 and 1. We again adopt the findings of the Radford et al. so that we use \n",
    "\n",
    "1. batch normalisation\n",
    "2. leakly-ReLU with coefficient 0.2\n",
    "3. convolutional layers with strides to downsample (instead of dense layers).\n",
    "\n",
    "Again, we use different dimensions from the figure above to meet our requirements. We only use one dense layer to get our final predicted probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriminator(x, reuse=False):\n",
    "    with tf.variable_scope(\"descriminator\") as scope:\n",
    "        if reuse:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        \n",
    "        x_reshape = tf.reshape(x, shape=(-1, 28, 28, 1))\n",
    "\n",
    "        # To 14,14\n",
    "        w_conv1 = tf.get_variable('d_w1', shape=[5, 5, 1, 16], initializer=tf.initializers.truncated_normal(stddev=0.2))\n",
    "        b_conv1 = tf.get_variable('d_b1', shape=[16], initializer=tf.initializers.zeros)\n",
    "        conv_1 = tf.nn.conv2d(x_reshape, filter=w_conv1, strides=[1,2,2,1], padding='SAME')\n",
    "        conv_1 = tf.contrib.layers.batch_norm(conv_1 + b_conv1, epsilon=1e-5, scope='dbn1')\n",
    "        conv_1 = tf.nn.leaky_relu(conv_1)\n",
    "\n",
    "        # To 7,7\n",
    "        w_conv2 = tf.get_variable('d_w2', shape=[5, 5, 16, 32], initializer=tf.initializers.truncated_normal(stddev=0.2))\n",
    "        b_conv2 = tf.get_variable('d_b2', shape=[32], initializer=tf.initializers.zeros)\n",
    "        conv_2 = tf.nn.conv2d(conv_1, filter=w_conv2, strides=[1,2,2,1], padding='SAME')\n",
    "        conv_2 = tf.contrib.layers.batch_norm(conv_2 + b_conv2, epsilon=1e-5, scope='dbn2')\n",
    "        conv_2 = tf.nn.leaky_relu(conv_2)\n",
    "\n",
    "        # To 4,4\n",
    "        w_conv3 = tf.get_variable('d_w3', shape=[5, 5, 32, 64], initializer=tf.initializers.truncated_normal(stddev=0.2))\n",
    "        b_conv3 = tf.get_variable('d_b3', shape=[64], initializer=tf.initializers.zeros)\n",
    "        conv_3 = tf.nn.conv2d(conv_2, filter=w_conv3, strides=[1,2,2,1], padding='SAME')\n",
    "        conv_3 = tf.contrib.layers.batch_norm(conv_3 + b_conv3, epsilon=1e-5, scope='dbn3')\n",
    "        conv_3 = tf.nn.leaky_relu(conv_3)\n",
    "\n",
    "        # To 1,1\n",
    "        w_conv4 = tf.get_variable('d_w4', shape=[3, 3, 64, 128], initializer=tf.initializers.truncated_normal(stddev=0.2))\n",
    "        b_conv4 = tf.get_variable('d_b4', shape=[128], initializer=tf.initializers.zeros)\n",
    "        conv_4 = tf.nn.conv2d(conv_3, filter=w_conv4, strides=[1,2,2,1], padding='VALID')\n",
    "        conv_4 = tf.contrib.layers.batch_norm(conv_4 + b_conv4, epsilon=1e-5, scope='dbn4')\n",
    "        conv_4 = tf.nn.leaky_relu(conv_4)\n",
    "        conv_4 = tf.reshape(conv_4, shape=[-1, 128])\n",
    "        \n",
    "        #Dense layer 128 to 1\n",
    "        w_d1 = tf.get_variable('d_wd1', shape=[128, 1], initializer=tf.initializers.truncated_normal(stddev=0.2))\n",
    "        b_d1 = tf.get_variable('d_db1', shape=[1], initializer=tf.initializers.zeros)\n",
    "        out_p = tf.matmul(conv_4, w_d1) + b_d1\n",
    "        return out_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Functions and Optimiser\n",
    "***\n",
    "### 4.1 Generator Loss\n",
    "The aim of the generator network is to get a high probability value D(.) of the discriminator for its generated images G(z). Applying gradient descent, the generator descents according to the following minibatch gradient descent formula where m is the minibatch size:\n",
    "![Generator Loss](imgs/loss_g.jpeg)\n",
    "<sub>Image Source: [Original GAN Paper](https://arxiv.org/abs/1406.2661)</sub> \n",
    "\n",
    "### 4.2 Discriminator Loss\n",
    "The discriminator combines two different loss functions. First, the probability values should be maximised for the real images, D(x), and second, the probability values should be minimised for the fake images, D(G(z)), or the complimentary value, 1 - D(G(z)), should be maximised. In the gradient descent framework, this means that we ascent according to the minibatch gradient ascent formula:\n",
    "![Discriminator Loss](imgs/loss_d.jpeg)\n",
    "<sub>Image Source: [Original GAN Paper](https://arxiv.org/abs/1406.2661)</sub> \n",
    "\n",
    "In tensorflow, we can use the function sigmoid_cross_entropy_with logits for all three cases. The reason is that a.) the sigmoid assures an output value between 0 and 1, and b.) the cross entropy loss is reduced to either log(1 - D(G(z)) (generator case), log(D(G(z)) (first discriminator case), or log(1 - D(x)) (second discriminator case) as we set p = 1 (1. and 3. case) or p = 0 (2. case) in the cross entropy formula. If this was too complicated, you might get a better understanding of loss functions in GANs [here](https://danieltakeshi.github.io/2017/03/05/understanding-generative-adversarial-networks/).\n",
    "\n",
    "### 4.3 Optimiser\n",
    "I implemented the Adam optimiser with a learning rate of 0.0002, as suggested by Redford et al. in the DCGAN paper. An interesting detail is that we have to define the variables we would like to optimise over, as we update the discriminator and generator in turns. This is why we used variables names starting with 'g_' or 'd_', respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # To ensure that everything we did is deleted\n",
    "\n",
    "batch_size = 128\n",
    "z = tf.placeholder(dtype=tf.float32, shape=[None, z_dimension])\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, 784])\n",
    "Gen_z = generator(z, batch_size, z_dim=z_dimension)\n",
    "Des_gz = descriminator(Gen_z)\n",
    "Des_x = descriminator(x, reuse=True)\n",
    "\n",
    "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(Des_gz), logits=Des_gz))\n",
    "d_loss1 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(Des_gz), logits=Des_gz))\n",
    "d_loss2 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(Des_x), logits=Des_x))\n",
    "d_loss = d_loss1 + d_loss2\n",
    "\n",
    "train_var = tf.trainable_variables()\n",
    "g_var = [v for v in train_var if 'g_' in v.name]\n",
    "d_var = [v for v in train_var if 'd_' in v.name]\n",
    "g_optimiser = tf.train.AdamOptimizer(0.0002).minimize(g_loss, var_list=g_var)\n",
    "d_optimiser = tf.train.AdamOptimizer(0.0002).minimize(-1*d_loss, var_list=d_var) # minimizing -1*d_loss is the same as maximizing d_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the GANs\n",
    "***\n",
    "Finally, we can train the GANs! The only thing we have to keep in mind is that we train the two networks in turns, and that we optimise over the approriate discriminator / generator weights only!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Import Data\n",
    "First, we import the real images. I use the CSV file of the training images, however, you could also use the standard images. You can access the CSV version [here](https://pjreddie.com/projects/mnist-in-csv/), or the ordinary version [here](http://yann.lecun.com/exdb/mnist/). I save the array as a pickle file to ensure faster use later. **Its is important to note** that we need to rescale the data to the tanh range (last Generator activation function) which is -1 to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded from pickle file.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open('train_data.p', 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    print('Training data loaded from pickle file.')\n",
    "except:\n",
    "    train_d = genfromtxt('mnist_train.csv', delimiter=',')\n",
    "    train_data = train_d[:, 1:]\n",
    "    with open('train_data.p', 'wb') as f:\n",
    "        pickle.dump(train_data, f)\n",
    "    print('Training Data loaded and saved as pickle file.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = (train_data-128)/128 # Scale the data appropriately to -1-1\n",
    "example_img = train_data[np.random.randint(0, len(train_data))].reshape((28,28))\n",
    "plt.imshow(example_img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Training Loop\n",
    "The training loop is very simple - we perform minibatch gradient descent with a batch size of 128 and 5000 iterations. We call the optimiser in every iteration, and log the losses in every 200th iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loss ( 0 Epoch) : 0.7891028\n",
      "Descriminator loss ( 0 Epoch) : 1.7098527\n",
      "Generator loss ( 200 Epoch) : 2.4038632\n",
      "Descriminator loss ( 200 Epoch) : 8.266841\n",
      "Generator loss ( 400 Epoch) : 6.0672216\n",
      "Descriminator loss ( 400 Epoch) : 13.951612\n",
      "Generator loss ( 600 Epoch) : 8.075574\n",
      "Descriminator loss ( 600 Epoch) : 18.288834\n",
      "Generator loss ( 800 Epoch) : 10.121113\n",
      "Descriminator loss ( 800 Epoch) : 22.132442\n",
      "Generator loss ( 1000 Epoch) : 11.1042595\n",
      "Descriminator loss ( 1000 Epoch) : 23.923069\n",
      "Generator loss ( 1200 Epoch) : 12.250232\n",
      "Descriminator loss ( 1200 Epoch) : 27.644455\n",
      "Generator loss ( 1400 Epoch) : 14.345179\n",
      "Descriminator loss ( 1400 Epoch) : 30.837286\n",
      "Generator loss ( 1600 Epoch) : 15.048914\n",
      "Descriminator loss ( 1600 Epoch) : 33.41051\n",
      "Generator loss ( 1800 Epoch) : 15.73361\n",
      "Descriminator loss ( 1800 Epoch) : 35.286335\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "train_data_l = len(train_data)\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(5000):\n",
    "    batch_x = train_data[np.random.choice(train_data_l, size=batch_size, replace=False)]\n",
    "    batch_z = np.random.normal(size=(batch_size, z_dimension))\n",
    "    feed_dict = {x: batch_x, z: batch_z}\n",
    "    _, dl = session.run([d_optimiser, d_loss], feed_dict=feed_dict)\n",
    "    _, gl = session.run([g_optimiser, g_loss], feed_dict=feed_dict)\n",
    "    if i%200 == 0:\n",
    "        print('Generator loss (', i, 'Epoch) :', gl)\n",
    "        print('Descriminator loss (', i, 'Epoch) :', dl)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results\n",
    "Let's have a look at a random result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img = generator(z,1,z_dim=z_dimension, reuse=True)\n",
    "z_batch = np.random.normal(size=(1, z_dimension))\n",
    "img = session.run([sample_img], feed_dict={z: z_batch})\n",
    "img = np.reshape(img, (28,28))\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a 20x20 image of 20x20=400 example digits to get a better overview over the quality of our generated images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_img = np.zeros((560,560), dtype=np.uint8)\n",
    "for i in range(20):\n",
    "    for j in range(20):\n",
    "        sample_img = generator(z,1,z_dim=z_dimension, reuse=True)\n",
    "        z_batch = np.random.normal(size=(1, z_dimension))\n",
    "        img = session.run([sample_img], feed_dict={z: z_batch})\n",
    "        img = np.reshape(img, (28,28))\n",
    "        img = (img*128+128).round().astype(np.uint8)\n",
    "        final_img[i*28:(i+1)*28, j*28:(j+1)*28] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(final_img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Thoughts\n",
    "I hope you enjoyed the quick introduction to GANs! Generative models are becoming more important and have great potential in many regards, and GANs are one of the most popular architectures right now.\n",
    "\n",
    "If you have any suggestions/comments/questions, feel free to contact me!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensor-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
